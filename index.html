<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
	button {
		margin-top: 150px;
		width: 1000px;
		height: 500px;
		border: 0;
		cursor: pointer;
		font-size: 2rem;
		color: white;
		background-image: url('./resources/unselected_base.png');
		background-size: cover;
	}
	button:hover {
		background-image: url('./resources/unselected_ours.png');
		background-size: cover;
	}
    </style>
</style>

<html>
<head>
	<title>Improving Sample Quality of Diffusion Model Using the Self-Attention Guidance</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Improving Sample Quality of Diffusion Model Using Self-Attention Guidance" />
<!-- 	<meta property="og:description" content="Paper description." /> -->

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Improving Sample Quality of Diffusion Model Using<br>Self-Attention Guidance</span>
		<table align=center width=600px>
			<table align=center width=600px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://pndong.github.io">Susung Hong</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.com/citations?user=O6B36wwAAAAJ&hl=ko&oi=sra">Gyuseong Lee</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="">Wooseok Jang</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://cvlab.korea.ac.kr/members">Seungryong Kim</a></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=800px>
				<tr>
					<td align=center width=150px>
						<center>
							<span style="font-size:20px">Korea University, Seoul, Korea</a></span>
						</center>
					</td>

				</tr>
			</table>
			<hr>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:20px"><a href="https://arxiv.org/abs/2210.00939">[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:20px"><a href="https://github.com/KU-CVLAB/Self-Attention-Guidance/">[Code]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>
	<hr>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:1000px" src="./resources/teaser.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					Qualitative comparisons between unguided (top row) and self-attention-guided (ours, bottom row) samples. Without any further training, image label or additional module, self-attention guidance can guide pretrained diffusion models to generate images with the higher quality. It is based on the finding that the self-attention maps in pretrained diffusion models are highly related to quality of the generated images. For detailed explanation, please refer to our <a href="https://arxiv.org/abs/2210.00939">Paper and Supplementary Material</a>.
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Following generative adversarial networks (GANs), a de facto standard model for image generation, denoising diffusion models (DDMs) have been actively researched and attracted strong attention due to their capability to generate images with high quality and diversity. However, the way the internal self-attention mechanism works inside the UNet of DDMs is under-explored. To unveil them, in this paper, we first investigate the self-attention operations within the black-boxed diffusion models and build hypotheses. Next, we verify the hypotheses about the self-attention map by conducting frequency analysis and testing the relationships with the generated objects. In consequence, we find out that the attention map is closely related to the quality of generated images. On the other hand, diffusion guidance methods based on additional information such as labels are proposed to improve the quality of generated images. Inspired by these methods, we present label-free guidance based on the intermediate self-attention map that can guide existing pretrained diffusion models to generate images with higher fidelity. In addition to the enhanced sample quality when used alone, we show that the results are further improved by combining our method with classifier guidance on ImageNet 128x128.
			</td>
		</tr>
	</table>
	<br>

	<hr>
	

	<center><h1>Animated Results</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:500px" src="./resources/ani1.gif"/></td>
				</center>
			</td>
			
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:500px" src="./resources/ani2.gif"/></td>
				</center>
			</td>
		</tr>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:500px" src="./resources/ani3.gif"/></td>
				</center>
			</td>
			
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:500px" src="./resources/ani4.gif"/></td>
				</center>
			</td>
		</tr>
	</table>

	<div class="box"></div>
	
	<table align=center width=400px>
		<center><button type="submit">
			Put on your mouse!
		</button></center>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<center>Unselected qualitative comparison of our method to unguided ADM.</center>
				</td>
			</tr>
		</center>
	</table>
	<br>

	<hr>
	<center><h1>Exploring Self-Attention Maps<br>Inside Diffusion Models</h1></center>

	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:1000px" src="./resources/compare.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<center>Comparison of self-attention masks. (a) Self-attention masks of DINO (Caron et al. 2021), (b) self-attention masks
						of unconditional ADM (Dhariwal and Nichol 2021). Compared to the self-attention masks of the DINO, those of ADM tend
						to attend to the high-frequency details, e.g., edge of an object, bushes, pebbles, and other repetitive patterns, rather than being
						centric on a single object.</center>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:700px" src="./resources/frequency.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=950px>
		<center>
			<tr>
				<td width=700px>
					<center>Frequency analysis of the self-attention map.</center>
				</td>
			</tr>
		</center>
	</table>
	<br>

	<hr>
	<center><h1>Method Diagram</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:1000px" src="./resources/method_diagram.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<br>

	<hr>

	
	<center><h1>Quantitative Results</h1></center>

	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:700px" src="./resources/main_table.png"/></td>
				</center>
			</td>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:300px" src="./resources/sub_table.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<center>Quantitative results of our method.</center>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					For the more detailed method, results, analyses, and powerful ablation study, please refer to our <a href="https://arxiv.org/abs/2210.00939">Paper and Supplementary Material</a>!
				</td>
			</tr>
		</center>
	</table>
	<br>

	<hr>
	<table align=center width=450px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">S. Hong, G. Lee,<br> W. Jang, S. Kim.<br>
				<b>Improving Sample Quality of Diffusion Model Using Self-Attention Guidance</b><br>
				(hosted on <a href="https://arxiv.org/abs/2210.00939">ArXiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

